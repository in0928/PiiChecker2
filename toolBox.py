import codecs
import os
import re
import csv
import chardet
import emoji
import pandas as pd
import neologdn
import spacy


def find_encoding(file):
    r_file = open(file, 'rb').read()
    result = chardet.detect(r_file)
    charenc = result['encoding']
    return charenc


def read_to_df(file, encode):
    with codecs.open(file, "r", encode, "ignore") as f:
        df = pd.read_table(f, delimiter="\t")
    return df


def read_to_list(file, encode):
    with open(file, 'r', encoding=encode) as f:
        reader = csv.reader(f)
        word_list = list(reader)
        return word_list


def filtered_df(df):
    new_df = df[~df["é€ä¿¡è€…ID[msg.userId]"].str.contains("dummy-", na=False)]
    return new_df


def pre_process(text_list):
    result = []
    for text in text_list:
        if not isinstance(text, str):
            text = str(text)
        normalized_text = neologdn.normalize(text)
        text_no_url = re.sub(r'https?://[\w/:%#\$&\?\(\)~\.=\+\-]+', '', normalized_text)
        text_no_emoji = ''.join(['' if c in emoji.UNICODE_EMOJI else c for c in text_no_url])
        tmp = re.sub(r'[!-/:-@[-`{-~]', r' ', text_no_emoji)
        text_removed_symbol = re.sub(u'[â– -â™¯]', ' ', tmp)
        result.append(text_removed_symbol)
    return result


def add_stop_words(customize_stop_words, nlp):
    for w in customize_stop_words:
        nlp.vocab[w[0]].is_stop = True


def write_to_csv(df, output_filename):
    df.to_csv(output_filename, sep="\t")
    print("Successfully wrote df to csv file")


def write_to_xlsx(df, output_filename):
    with pd.ExcelWriter(output_filename) as writer:
        df.to_excel(writer, sheet_name=output_filename)
    print("Successfully wrote df to excel file")


if __name__=="__main__":
    stop_keys = "C:\\Users\\Ko.In\\Desktop\\PiiExtractionData\\StopKey_v01.csv"
    # file_path = "C:\\Users\\Ko.In\\Desktop\\testdata.csv"
    file_path = "C:\\Users\\Ko.In\\Desktop\\PiiExtractionData\\callcenter_data (201809).csv"
    df = read_to_df(file_path, find_encoding(file_path))
    df = filtered_df(df)

    # text = "ãŠå‹é”ã®ç´¹ä»‹ã§ã€å¥³å­ï¼’äººã§ä¸‰æ™‚ã®ãƒ†ã‚£ãƒ¼ã‚¿ã‚¤ãƒ ã«åˆ©ç”¨ã—ã¾ã—ãŸã€‚2äººç”¨ã®ã‚½ãƒ•ã‚¡ã«ä¸¦ã‚“ã§ã„ãŸã ãã¾ã€œã™ v(^^)v ãªã‹ã‚ˆã—ï¼ˆç¬‘" \
    #        "æœ€å¾Œã«å‡ºã•ã‚ŒãŸ,ãƒ¢ãƒ³ãƒ–ãƒ©ãƒ³ã®ï½¹ï½°ï½·ã€‚" \
    #        "ã‚„ã°ã£ã£ã£ï¼ï¼ã“ã‚Œã¯ã†ã¾ãƒ¼ãƒ¼ãƒ¼ã„!!" \
    #        "ã¨ã£ã¦ã‚‚ï¼¤ï½…ï½Œï½‰ï½ƒï½‰ï½ï½•ï½“ã§ã€ã‚µãƒ¼ãƒ“ã‚¹ã‚‚Goodã§ã—ãŸğŸ˜€" \
    #        "ã“ã‚Œã§2,500å††ã¯ã¨ã£ã¦ã‚‚ãŠå¾—ã§ã™â˜†" \
    #        "http://hogehoge.nantoka.blog/example/link.html"


