import re

import neologdn
import spacy

class TextChecker:

    @staticmethod
    def checkName(nlp_sentence):
        name = []
        count = 0
        while count < len(nlp_sentence):
            if nlp_sentence[count].is_stop:
                # print("Found stop_word:" + str(nlp_sentence[count]))
                count += 1
                continue
            if nlp_sentence[count]._.pos_detail == "åè©,å›ºæœ‰åè©,äººå,å§“":
                # to detect full-name
                if count < len(nlp_sentence)-1 and nlp_sentence[count+1]._.pos_detail == "åè©,å›ºæœ‰åè©,äººå,å":
                    name.append((nlp_sentence[count], nlp_sentence[count+1]))
                    count += 1
                else:
                    name.append(nlp_sentence[count])
            elif nlp_sentence[count]._.pos_detail == "åè©,å›ºæœ‰åè©,äººå,å":
                name.append(nlp_sentence[count])
            count += 1
        return name

    @staticmethod
    def checkLocation(nlp_sentence):
        location = []
        address = []
        count = 0
        while count < len(nlp_sentence):
            if nlp_sentence[count].is_stop:
                # print("Found stop_word:" + str(nlp_sentence[count]))
                count += 1
                continue
            if nlp_sentence[count]._.pos_detail == "åè©,å›ºæœ‰åè©,åœ°å,ä¸€èˆ¬":
                # detect all location entity
                end_index = TextChecker.target_ends_at(nlp_sentence[count+1:], "åè©,å›ºæœ‰åè©,åœ°å,ä¸€èˆ¬") + count
                location.append(nlp_sentence[count: end_index+1])
                # following location entities exist
                if end_index > count:
                    count = end_index

                number_end_index = TextChecker.number_ends_at(nlp_sentence[count+1:]) + count
                print("number ends at: " + str(TextChecker.number_ends_at(nlp_sentence[count+1:])))
                print(number_end_index)
                # number entities exist
                if number_end_index > count:
                    # location.append(nlp_sentence[end_index+1: number_end_index+1])
                    [address.append(i) for i in location]
                    address.append(nlp_sentence[end_index+1: number_end_index+1])

            count += 1
        return location, address


    @staticmethod
    def number_ends_at(tokens):
        count = 0
        while count < len(tokens):
            if tokens[count]._.pos_detail == "åè©,æ•°è©,*,*":
                count += 2
                continue
            elif count > 0 and tokens[count-3]._.pos_detail == "è£œåŠ©è¨˜å·,ä¸€èˆ¬,*,*": #TODO: amazon 3000en
                return count-1
            else:
                return count
        return count

    @staticmethod
    def target_ends_at(tokens, target):
        """
        :param tokens: a list of words which can be indexed
        :param target: _.pos_detail of the word in tokens
        :param target_ignore: _.pos_detail of token to ignore, optional
        :return: the index where the last match with target tag_ occurs
        """
        count = 0
        while count < len(tokens):
            if tokens[count]._.pos_detail == target:
                count += 1
                continue
            else:
                return count
        return count


if __name__=="__main__":
    s1 = "æ•°å­—ãŒ4ã¤ã§æ–‡å­—ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒè¤‡æ•°ã®å ´åˆã€æ±äº¬éƒ½ç«‹å·å¸‚æ¸¯åŒºä¸Šæœ¨è‘‰ä¸‹ç”º5-1-3-1502ã ã‚ˆ"
    s2 = "æ•°å­—ãŒ4ã¤ã§æ–‡å­—ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒ1ã¤ã®å ´åˆã€å…­æœ¬æœ¨5-1-3-1122ã ã‚ˆ"
    s3 = "æ•°å­—ãŒ3ã¤ã§æ–‡å­—ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒè¤‡æ•°ã®å ´åˆã€æ±äº¬éƒ½å¤§é˜ªå¸‚æµªé€ŸåŒºãªã‚“ã°5-1-3ã ã‚ˆ"
    s4 = "æ•°å­—ãŒ3ã¤ã§æ–‡å­—ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒ1ã¤ã®å ´åˆã€å…­æœ¬æœ¨ï¼’ãƒ¼ï¼“ãƒ¼ï¼“ã ã‚ˆ"
    s5 = "æ•°å­—ãŒ2ã¤ã§æ–‡å­—ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒè¤‡æ•°ã®å ´åˆã€æ±äº¬éƒ½ç«‹å·å¸‚æ¸¯åŒºå…­æœ¬æœ¨ï¼‘ï¼ï¼“[ã ã‚ˆ"
    s6 = "æ•°å­—ãŒ2ã¤ã§æ–‡å­—ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒ1ã¤ã®å ´åˆã€éº»å¸ƒåç•ªï¼“ï¼ï¼“ã ã‚ˆ"
    s7 = "æ•°å­—ãŒæ¼¢æ•°å­—ã§æ–‡å­—ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒè¤‡æ•°ã®å ´åˆã€æ±äº¬éƒ½ç«‹å·å¸‚æ¸¯åŒºå®¿æ¯›ä¸‰ä¸ç›®äºŒç•ªåœ°äº”å·ã ã‚ˆ"
    s8 = "æ•°å­—ãŒæ¼¢æ•°å­—ã§æ–‡å­—ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒè¤‡æ•°ã®å ´åˆã€æ±äº¬éƒ½ç«‹å·å¸‚æ¸¯åŒºæµœæ¾ç”ºä¸‰ä¸ç›®äºŒç•ªåœ°ã ã‚ˆ"
    s9 = "æ•°å­—ãŒæ¼¢æ•°å­—ã§æ–‡å­—ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒ1ã¤ã®å ´åˆã€æ±äº¬éƒ½ç«‹å·å¸‚æ¸¯åŒºã ã‚ˆ"

    test_list = [s1,s2,s3,s4,s5,s6,s7,s8,s9]
    nlp = spacy.load('ja_ginza_nopn', disable=["tagger", "parser", "ner", "textcat"])

    for s in test_list:
        s = nlp(s)
        address = TextChecker.checkLocation(s)
        print(address)

    # text = "ãŠå‹é”ã®ç´¹ä»‹ã§ã€å¥³å­ï¼’äººã§ä¸‰æ™‚ã®ãƒ†ã‚£ãƒ¼ã‚¿ã‚¤ãƒ ã«åˆ©ç”¨ã—ã¾ã—ãŸã€‚2äººç”¨ã®ã‚½ãƒ•ã‚¡ã«ä¸¦ã‚“ã§ã„ãŸã ãã¾ã€œã™ v(^^)v ãªã‹ã‚ˆã—ï¼ˆç¬‘" \
    #        "æœ€å¾Œã«å‡ºã•ã‚ŒãŸ,ãƒ¢ãƒ³ãƒ–ãƒ©ãƒ³ã®ï½¹ï½°ï½·ã€‚" \
    #        "ã‚„ã°ã£ã£ã£ï¼ï¼ã“ã‚Œã¯ã†ã¾ãƒ¼ãƒ¼ãƒ¼ã„!!" \
    #        "ã¨ã£ã¦ã‚‚ï¼¤ï½…ï½Œï½‰ï½ƒï½‰ï½ï½•ï½“ã§ã€ã‚µãƒ¼ãƒ“ã‚¹ã‚‚Goodã§ã—ãŸAmazonğŸ˜€" \
    #        "ã“ã‚Œã§2,500å††ã¯ã¨ã£ã¦ã‚‚ãŠå¾—ã§ã™â˜†" \
    #        "http://hogehoge.nantoka.blog/example/link.html"
    #
    # test = "Amazon3-3-31["
    # nlp = spacy.load('ja_ginza_nopn', disable=["tagger", "parser", "ner", "textcat"])
    # nlp_sentence = nlp(test)
    # print(nlp_sentence[1:])
    #
    #
    # print(TextChecker.number_ends_at(nlp_sentence[1:]))

